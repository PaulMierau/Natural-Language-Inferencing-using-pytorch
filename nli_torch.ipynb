{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Intro2DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulMierau/natural-language-inferencing-using-pytorch/blob/main/nli_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wihcz1ShrAc"
      },
      "source": [
        "Set your drive path to datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6VZcAFZhpTX"
      },
      "source": [
        "# Important:\r\n",
        "# Download the train.csv and test.csv from https://www.kaggle.com/c/contradictory-my-dear-watson/data\r\n",
        "# and make sure they are uploaded to your working direction, specified below\r\n",
        "# !!! Running this notebook might require a colab pro account with high-ram runtime selection !!!\r\n",
        "work_dir = \"/content/drive/MyDrive/Intro2DL/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_NMDhMKDwvY"
      },
      "source": [
        "import numpy as np \r\n",
        "import pandas as pd \r\n",
        "\r\n",
        "%load_ext tensorboard\r\n",
        "from tensorflow import summary\r\n",
        "import tensorflow as tf\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import os\r\n",
        "!pip3 install transformers\r\n",
        "!pip3 install datasets\r\n",
        "!pip3 install sentencepiece\r\n",
        "from transformers import  XLMRobertaTokenizer, AutoTokenizer, AutoModelForSequenceClassification, AdamW\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import torch.optim as optim\r\n",
        "from datasets import load_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO_rKMZoKtGi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkaX_1Te5Poy"
      },
      "source": [
        "# Define training dataset from train.csv and MNLI source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H62xY7A5Fc6k"
      },
      "source": [
        "class SentencesDataset(Dataset):\r\n",
        "    def __init__(self, data_path = work_dir + \"train.csv\", model_name = \"joeddav/xlm-roberta-large-xnli\", maxLength = 200, isTest=False):\r\n",
        "      self.isTest = isTest\r\n",
        "      data = pd.read_csv(data_path)\r\n",
        "      tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\r\n",
        "      self.encoded_data = tokenizer(list(data.premise.values), list(data.hypothesis.values),truncation=True, padding = True, return_tensors=\"pt\")\r\n",
        "      mask = torch.tensor([1 if entry[maxLength] == 1 else 0 for entry in self.encoded_data['input_ids']])\r\n",
        "      for key in list(self.encoded_data.keys()):\r\n",
        "        self.encoded_data[key] = self.encoded_data[key][torch.nonzero(mask)]\r\n",
        "        self.encoded_data[key] = self.encoded_data[key].reshape(self.encoded_data[key].shape[0],self.encoded_data[key].shape[2])[:,:maxLength]\r\n",
        "      if not isTest:\r\n",
        "        self.encoded_data[\"labels\"] = data.label.values[torch.nonzero(mask)]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.encoded_data[\"input_ids\"])\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "      if self.isTest:\r\n",
        "        return {\"input_ids\":self.encoded_data[\"input_ids\"][idx],\\\r\n",
        "                \"attention_mask\": self.encoded_data[\"attention_mask\"][idx]}\r\n",
        "\r\n",
        "\r\n",
        "      return {\"input_ids\":self.encoded_data[\"input_ids\"][idx],\\\r\n",
        "              \"attention_mask\": self.encoded_data[\"attention_mask\"][idx],\\\r\n",
        "              \"labels\": self.encoded_data[\"labels\"][idx]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYx51adwFxoV"
      },
      "source": [
        "def load_mnli():\r\n",
        "  data = {\"hypotheses\": [], \"premises\": [], \"labels\":[]}\r\n",
        "  dataset = load_dataset(\"multi_nli\")\r\n",
        "  for idx, sample in enumerate(dataset[\"train\"]):\r\n",
        "    # choose number of training samples to avoid memory issues or speedup training\r\n",
        "    if idx > 10000:\r\n",
        "      break\r\n",
        "    data[\"hypotheses\"].append(sample[\"hypothesis\"])\r\n",
        "    data[\"premises\"].append(sample[\"premise\"])\r\n",
        "    data[\"labels\"].append(sample[\"label\"])\r\n",
        "  return data\r\n",
        "\r\n",
        "class MnliDataset(Dataset):\r\n",
        "    def __init__(self, model_name = \"joeddav/xlm-roberta-large-xnli\", maxLength = 200):\r\n",
        "      data = load_mnli()\r\n",
        "      tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\r\n",
        "      self.encoded_data = tokenizer(list(data[\"premises\"]), list(data[\"hypotheses\"]),max_length=512 ,padding = \"max_length\",  return_tensors=\"pt\",)\r\n",
        "      mask = torch.tensor([1 if entry[maxLength] == 1 else 0 for entry in self.encoded_data['input_ids']])\r\n",
        "      for key in list(self.encoded_data.keys()):\r\n",
        "        self.encoded_data[key] = self.encoded_data[key][torch.nonzero(mask)]\r\n",
        "        self.encoded_data[key] = self.encoded_data[key].reshape(self.encoded_data[key].shape[0],self.encoded_data[key].shape[2])[:,:maxLength]\r\n",
        "      self.encoded_data[\"labels\"] = np.array(data[\"labels\"])[torch.nonzero(mask)]\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "      return len(self.encoded_data[\"input_ids\"])\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "      return {\"input_ids\":self.encoded_data[\"input_ids\"][idx],\\\r\n",
        "                \"attention_mask\": self.encoded_data[\"attention_mask\"][idx],\\\r\n",
        "                \"labels\": self.encoded_data[\"labels\"][idx]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHr_PTXn6TgY"
      },
      "source": [
        "# Initialize datasets and split into 80% for training and 20% for validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLsrMD8DNwYz"
      },
      "source": [
        "batch_size = 64\r\n",
        "mnli = MnliDataset()\r\n",
        "sentences = SentencesDataset()\r\n",
        "full_dataset = torch.utils.data.ConcatDataset([mnli, sentences])\r\n",
        "train_size = int(0.8*len(full_dataset))\r\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, len(full_dataset)- train_size])\r\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\r\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZlL7Cq1ewYK"
      },
      "source": [
        "print(len(train_dataset), len(val_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilZonDCi3ms1"
      },
      "source": [
        "#Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKWZfeRGo4Mb"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "class LSTMRoBerta(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(LSTMRoBerta, self).__init__()\r\n",
        "    self.roberta = AutoModelForSequenceClassification.from_pretrained(\"joeddav/xlm-roberta-large-xnli\").base_model\r\n",
        "    for param in self.roberta.parameters():\r\n",
        "           param.requires_grad = False\r\n",
        "    self.lstm = nn.LSTM(1024, 512, bidirectional=True, batch_first=True)\r\n",
        "    self.classifier  = nn.Linear(1024, 3)\r\n",
        "    \r\n",
        "  def forward(self, input1, input2, hidden, train = True):\r\n",
        "    if train == True:\r\n",
        "      hidden = self.init_hidden(input1.shape[0])\r\n",
        "    out1 = self.roberta(input_ids=input1, attention_mask=input2)[0]\r\n",
        "    _, (h, c) = self.lstm(out1, hidden)\r\n",
        "    out2 = torch.cat((h[0], h[1]), dim=1)\r\n",
        "    pred = self.classifier(out2)\r\n",
        "    hidden = (h, c)\r\n",
        "    return F.softmax(pred, dim=1), hidden\r\n",
        "\r\n",
        "  def init_hidden(self, batch_size):\r\n",
        "        h = torch.zeros(2, batch_size, 512).to(device)\r\n",
        "        c = torch.zeros(2, batch_size, 512).to(device)\r\n",
        "        return (h, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKApNkWw57JA"
      },
      "source": [
        "# Create the model on the desired device and define the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0siXB7qJQf9"
      },
      "source": [
        "device = 'cuda'\r\n",
        "model = LSTMRoBerta()\r\n",
        "model = model.to(device)\r\n",
        "\r\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90CkQzj6y2a7"
      },
      "source": [
        "def trainLSTM(model, optimizer, sample, h, first = False):\r\n",
        "  \"\"\"one training step for LSTM which returns the loss, number of correct predictions and the last hidden state\"\"\"\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "  optimizer.zero_grad()\r\n",
        "  with torch.autograd.set_detect_anomaly(True):\r\n",
        "    input1 = sample[\"input_ids\"].long().to(device)\r\n",
        "    input2 = sample[\"attention_mask\"].long().to(device)\r\n",
        "    target = sample['labels'].long().to(device) \r\n",
        "\r\n",
        "    pred, h = model(input1, input2, h)\r\n",
        "\r\n",
        "    pred_loss = criterion(pred, target.squeeze())\r\n",
        "    _, predicted = torch.max(pred, 1) \r\n",
        "    num_correct = (predicted == target.squeeze()).sum().item()\r\n",
        "    pred_loss.backward()\r\n",
        "        \r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "  return pred_loss.item(), num_correct, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKgigjGHzBfX"
      },
      "source": [
        "def validateLSTM(model, sample, h):\r\n",
        "  \"\"\"one validation step for LSTM which returns the loss, number of correct predictions and the last hidden state\"\"\"\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "      input1 = sample[\"input_ids\"].long().to(device)\r\n",
        "      input2 = sample[\"attention_mask\"].long().to(device)\r\n",
        "\r\n",
        "      target = sample['labels'].long().to(device) \r\n",
        "      \r\n",
        "      pred, h = model(input1, input2, h)\r\n",
        "      pred_loss = criterion(pred, target[0])\r\n",
        "      _, predicted = torch.max(pred, 1) \r\n",
        "      num_correct = (predicted  == target).sum().item()\r\n",
        "\r\n",
        "  return pred_loss.item(), num_correct, h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvqhzHk_4eNa"
      },
      "source": [
        "## Use Tensorboard for live visualization of the training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AatJvDAIy7KH"
      },
      "source": [
        "train_log_dir = './runs/train'\r\n",
        "train_summary_writer = summary.create_file_writer(train_log_dir)\r\n",
        "val_log_dir = './runs/validate'\r\n",
        "val_summary_writer = summary.create_file_writer(val_log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDYMnVrAKdtG"
      },
      "source": [
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0iVjBHo4mPY"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1196uYpx8wE"
      },
      "source": [
        "max_epoch = 5\r\n",
        "save_stride = 10\r\n",
        "max_accu = -1\r\n",
        "\r\n",
        "for epoch in tqdm(range(max_epoch)):        \r\n",
        "    train_loss = 0.0\r\n",
        "    train_accu = 0.0\r\n",
        "\r\n",
        "    first = True\r\n",
        "    h = model.init_hidden(batch_size)\r\n",
        "    with tqdm(total=len(train_dataloader)) as pbar:\r\n",
        "        for idx, sample in enumerate(train_dataloader):\r\n",
        "            h = tuple([each.data for each in h])\r\n",
        "            curr_loss, num_correct, h = trainLSTM(model, optimizer, sample, h, first)\r\n",
        "            train_loss += curr_loss / len(train_dataloader)\r\n",
        "            train_accu += num_correct / len(train_dataset)\r\n",
        "            pbar.update(1)\r\n",
        "            first = False\r\n",
        "\r\n",
        "    with train_summary_writer.as_default():\r\n",
        "        tf.summary.scalar('loss', train_loss, step=epoch)                \r\n",
        "        tf.summary.scalar('accuracy', train_accu, step=epoch)                \r\n",
        "\r\n",
        "    checkpoint = {\r\n",
        "        #'model' : DenseRoBerta(),\r\n",
        "        'model_state_dict': model.state_dict(),\r\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "    }\r\n",
        "    \r\n",
        "    val_loss = 0.0\r\n",
        "    val_accu = 0.0\r\n",
        "\r\n",
        "    h = model.init_hidden(1)\r\n",
        "    with tqdm(total=len(val_dataloader)) as pbar:\r\n",
        "        for idx, sample in enumerate(val_dataloader):\r\n",
        "            h = tuple([each.data for each in h])\r\n",
        "            curr_loss, num_correct, h = validateLSTM(model, sample, h)\r\n",
        "            val_loss += curr_loss / len(val_dataloader)\r\n",
        "            val_accu += num_correct / len(val_dataloader)\r\n",
        "            pbar.update(1)\r\n",
        "\r\n",
        "    with val_summary_writer.as_default():\r\n",
        "        tf.summary.scalar('loss', val_loss, step=epoch)\r\n",
        "        tf.summary.scalar('accuracy', val_accu, step=epoch) \r\n",
        "\r\n",
        "    max_accu = max(val_accu, max_accu)\r\n",
        "    if max_accu == val_accu:\r\n",
        "        torch.save(checkpoint, os.path.join(work_dir, 'roberta_dense_best.pth'))\r\n",
        "\r\n",
        "    print(train_accu, val_accu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm5mfXB-42tq"
      },
      "source": [
        "# Define test dataset from test.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtFaOcE6dfh_"
      },
      "source": [
        "class SentencesTestDataset(Dataset):\r\n",
        "    def __init__(self, data_path = work_dir + \"test.csv\", model_name = \"joeddav/xlm-roberta-large-xnli\", max_length=512):\r\n",
        "      data = pd.read_csv(data_path)\r\n",
        "      tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
        "      mask = torch.ones(5195)\r\n",
        "      self.encoded_data = tokenizer(list(data.premise.values), list(data.hypothesis.values),truncation=True, padding = True, return_tensors=\"pt\")\r\n",
        "      for key in list(self.encoded_data.keys()):\r\n",
        "        self.encoded_data[key] = self.encoded_data[key][torch.nonzero(mask)]\r\n",
        "        self.encoded_data[key] = self.encoded_data[key].reshape(self.encoded_data[key].shape[0],self.encoded_data[key].shape[2])\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.encoded_data[\"input_ids\"])\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return {\"input_ids\":self.encoded_data[\"input_ids\"][idx],\\\r\n",
        "                \"attention_mask\": self.encoded_data[\"attention_mask\"][idx]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxpd9m5lOMcF"
      },
      "source": [
        "test_dataset = SentencesTestDataset()\r\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yVME9Q64-hd"
      },
      "source": [
        "# Test loop and create submission for kaggle challenge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtrGSHFxPsCx"
      },
      "source": [
        "predictions = []\r\n",
        "test_loss = 0.0\r\n",
        "test_accu = 0.0\r\n",
        "checkpoint = torch.load(work_dir + \"roberta_dense_best.pth\")\r\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\r\n",
        "h = model.init_hidden(batch_size = batch_size)\r\n",
        "with tqdm(total=len(test_dataloader)) as pbar:\r\n",
        "        for idx, sample in enumerate(test_dataloader):\r\n",
        "            h = tuple([each.data for each in h])\r\n",
        "            with torch.no_grad():\r\n",
        "              model.eval()\r\n",
        "              criterion = nn.CrossEntropyLoss()\r\n",
        "              input1 = sample[\"input_ids\"].long().to(device)\r\n",
        "              input2 = sample[\"attention_mask\"].long().to(device)\r\n",
        "\r\n",
        "              pred, h = model(input1, input2, h)\r\n",
        "              _, predicted = torch.max(pred, 1)\r\n",
        "              predictions.append(predicted.item())\r\n",
        "              pbar.update(1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO7H_BhEPwB7"
      },
      "source": [
        "# The file submission.csv can be submitted on kaggle as a solution to the challenge\r\n",
        "\r\n",
        "data = pd.read_csv(work_dir + 'test.csv\")\r\n",
        "submission = data.id.copy().to_frame()\r\n",
        "submission['prediction'] = predictions\r\n",
        "submission.to_csv(\"submission.csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}